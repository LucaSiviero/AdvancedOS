MapReduce e le sue varianti sono riuscite a implementare con successo applicazioni a larga scala e data-intensive sui Commodity Cluster.
I commodity clusters sono classi di supercomputer al cui interno sono presenti diversi sistemi di calcolo connessi attraverso una rete di comunicazione.
Essi sono la classe dominante nel mondo dei sistemi commerciali scalabili. Nonostante non siano i più performanti, è molto probabile trovare un commodity cluster in una tipica stanza da super macchina.
Questi sistemi sono costruiti attorno a un modello di flusso dei dati aciclico, non adatto ad altre applicazioni abbastanza importanti. Il paper si concentra su questa classe di applicazioni, tra cui algoritmi iterativi di machine learning e strumenti di analisi dei dati interattiva. Sarà proposto un nuovo framework chiamato Spark che supporta queste applicazioni e, nel frattempo, assicura scalabilità e fault tolerance. Spark dispone di un'astrazione chiamata resilient distributed datasets (RDD). Un RDD è una collezione read-only di oggetti partizionati su un insieme di macchine. Se una partizione viene persa, l'oggetto partizionato può essere ripristinato ugualmente. Spark può performare fino a 10 volte meglio di Hadoop nei task di machine learning iterativo e può essere usato per interrogare un dataset di 39GB con un tempo di risposta inferiore al secondo.

1-Introduzione

